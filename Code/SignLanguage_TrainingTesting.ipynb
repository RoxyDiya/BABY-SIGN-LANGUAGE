{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BABY SIGN LANGUAGE MODEL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAINING THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.models import resnet18\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Configuration\n",
    "data_dir = r\"C:\\Users\\roksh\\OneDrive\\Desktop\\AI LAB\\SignLanguage\\Preprocessed_Frame_Hands\"  # Dataset directory\n",
    "num_classes = len(os.listdir(data_dir)) # Number of classes in the dataset\n",
    "input_shape = (3, 224, 224)\n",
    "batch_size = 32\n",
    "epochs = 15\n",
    "num_folds = 3  # Number of folds for K-Fold Cross-Validation ( we mostly used 2 or 3 folds for our experiments)\n",
    "\n",
    "def load_image(image_path): \n",
    "    image = cv2.imread(image_path)\n",
    "    return image\n",
    "\n",
    "# Dataset class for loading data from the directory\n",
    "\n",
    "class SignLanguageDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        self.data = []  # List of (frame_path, class_idx) tuples\n",
    "        self.transform = transform\n",
    "\n",
    "        class_mapping = {cls: idx for idx, cls in enumerate(sorted(os.listdir(data_dir)))}\n",
    "        for label_dir in os.listdir(data_dir):\n",
    "            label_path = os.path.join(data_dir, label_dir)\n",
    "            if os.path.isdir(label_path):\n",
    "                for video_dir in os.listdir(label_path):\n",
    "                    video_path = os.path.join(label_path, video_dir)\n",
    "                    if os.path.isdir(video_path):\n",
    "                        for frame_file in os.listdir(video_path):\n",
    "                            if frame_file.endswith(\".jpg\"):  # Filter for image files\n",
    "                                frame_path = os.path.join(video_path, frame_file)\n",
    "                                self.data.append((frame_path, class_mapping[label_dir]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        frame_path, class_idx = self.data[idx]\n",
    "        frame = load_image(frame_path)\n",
    "\n",
    "        if self.transform:\n",
    "            frame = self.transform(frame)\n",
    "\n",
    "        return frame, class_idx\n",
    "\n",
    "# ResNet-18 pretrained model where the final classification layer is removed and replaced with a custom classifier\n",
    "class ResNet18CustomClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ResNet18CustomClassifier, self).__init__()\n",
    "        self.resnet18 = resnet18(pretrained=True)\n",
    "        in_features = self.resnet18.fc.in_features\n",
    "\n",
    "        # Remove the final classification layer\n",
    "        self.resnet18.fc = nn.Identity()\n",
    "\n",
    "        # Add a custom classifier\n",
    "        self.custom_classifier = nn.Sequential(\n",
    "            nn.Linear(in_features, 256),  \n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5), \n",
    "            nn.Linear(256, num_classes)  \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.resnet18(x)\n",
    "        x = self.custom_classifier(x)\n",
    "        return x\n",
    "\n",
    "# Load and preprocess data\n",
    "transform = Compose([\n",
    "    ToTensor(),\n",
    "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "dataset = SignLanguageDataset(data_dir, transform=transform)\n",
    "kf = KFold(n_splits=num_folds, shuffle=True)\n",
    "\n",
    "train_acc_history = []\n",
    "val_acc_history = []\n",
    "\n",
    "# K-Fold Cross-Validation Loop\n",
    "fold = 0\n",
    "for train_indices, val_indices in kf.split(range(len(dataset))):\n",
    "    print(f\"Fold {fold}\")\n",
    "\n",
    "    # dataloaders for the current fold\n",
    "    train_sampler = torch.utils.data.SubsetRandomSampler(train_indices)\n",
    "    val_sampler = torch.utils.data.SubsetRandomSampler(val_indices)\n",
    "\n",
    "    train_dataloader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "    val_dataloader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)\n",
    "\n",
    "    # Create a new model instance for each fold\n",
    "    model = ResNet18CustomClassifier(num_classes)\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    best_val_accuracy = 0.0\n",
    "    best_model_state = None\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        all_labels = []\n",
    "        all_predictions = []\n",
    "\n",
    "        for inputs, labels in train_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.tolist())\n",
    "\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "            all_predictions.extend(predictions.tolist())\n",
    "\n",
    "        epoch_loss = total_loss / len(train_dataloader)\n",
    "        epoch_accuracy = accuracy_score(all_labels, all_predictions)\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        all_val_labels = []\n",
    "        all_val_predictions = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for val_inputs, val_labels in val_dataloader:\n",
    "                val_outputs = model(val_inputs)\n",
    "                val_batch_loss = criterion(val_outputs, val_labels)\n",
    "                val_loss += val_batch_loss.item()\n",
    "\n",
    "                _, val_predictions = torch.max(val_outputs, 1)\n",
    "                all_val_labels.extend(val_labels.tolist())\n",
    "                all_val_predictions.extend(val_predictions.tolist())\n",
    "\n",
    "            val_epoch_loss = val_loss / len(val_dataloader)\n",
    "            val_epoch_accuracy = accuracy_score(all_val_labels, all_val_predictions)\n",
    "\n",
    "        print(f\"Fold {fold+1}, Epoch [{epoch+1}/{epochs}], \"\n",
    "              f\"Training Loss: {epoch_loss:.4f}, Training Accuracy: {epoch_accuracy:.4f}, \"\n",
    "              f\"Validation Loss: {val_epoch_loss:.4f}, Validation Accuracy: {val_epoch_accuracy:.4f}\")\n",
    "        \n",
    "        # Append accuracy to history lists\n",
    "        train_acc_history.append(epoch_accuracy)\n",
    "        val_acc_history.append(val_epoch_accuracy)\n",
    "\n",
    "        if val_epoch_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_epoch_accuracy\n",
    "            best_model_state = model.state_dict()\n",
    "    \n",
    "    print(f\"Fold {fold+1} finished.\")\n",
    "    fold += 1\n",
    "    if best_model_state is not None:\n",
    "        fold_model_path = f'best_resnet18_fold_{fold+1}.pth'\n",
    "        torch.save(best_model_state, fold_model_path)\n",
    "        print(f\"Best model for fold {fold+1} saved to {fold_model_path}\")\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_acc_history, label='Training Accuracy')\n",
    "plt.plot(val_acc_history, label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Add fold labels\n",
    "for i in range(num_folds):\n",
    "    plt.text(i * epochs, max(val_acc_history), f'Fold {i+1}', fontsize=12, va='bottom', ha='left', backgroundcolor='w')\n",
    "\n",
    "plt.title('Training and Validation Accuracy History with Fold Labels')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "print(\"K-Fold Cross-Validation finished.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TESTING THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'copy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 53\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m     success, img \u001b[39m=\u001b[39m cap\u001b[39m.\u001b[39mread()\n\u001b[1;32m---> 53\u001b[0m     imgOutput \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39;49mcopy()\n\u001b[0;32m     54\u001b[0m     hands, img \u001b[39m=\u001b[39m detector\u001b[39m.\u001b[39mfindHands(img)\n\u001b[0;32m     56\u001b[0m     \u001b[39mif\u001b[39;00m hands:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'copy'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from cvzone.HandTrackingModule import HandDetector\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision.models import resnet18\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "\n",
    "class ResNet18CustomClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ResNet18CustomClassifier, self).__init__()\n",
    "        self.resnet18 = resnet18(pretrained=True)\n",
    "        in_features = self.resnet18.fc.in_features\n",
    "\n",
    "        self.resnet18.fc = nn.Identity()\n",
    "\n",
    "        self.custom_classifier = nn.Sequential(\n",
    "            nn.Linear(in_features, 256),  \n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),  \n",
    "            nn.Linear(256, num_classes) \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.resnet18(x)\n",
    "        x = self.custom_classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "labels = [\"dont_know\", \"down\", \"drink\",\"eat\", \"frustrated\", \"i_love_you\", \"mad_grumpy\", \"milk\", \"mine\", \"mom\", \"potty\", \"sorry\"]\n",
    "\n",
    "num_classes = len(labels)  # Number of classes in the dataset\n",
    "model = ResNet18CustomClassifier(num_classes)\n",
    "\n",
    "# Load the saved weights from the .pth file\n",
    "model.load_state_dict(torch.load(r\"C:\\Users\\roksh\\OneDrive\\Desktop\\AI LAB\\BabySignLanguageProject\\signlanguage_model.pth\"))\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "detector = HandDetector(maxHands=1)\n",
    "\n",
    "offset = 20\n",
    "imgSize = 224  \n",
    "\n",
    "\n",
    "font_color = (68,214,44)  \n",
    "font_scale = 1.4  \n",
    "font_thickness = 2  \n",
    "\n",
    "\n",
    "while True:\n",
    "    success, img = cap.read()\n",
    "    imgOutput = img.copy()\n",
    "    hands, img = detector.findHands(img)\n",
    "    \n",
    "    if hands:\n",
    "        hand = hands[0]\n",
    "        x, y, w, h = hand['bbox']\n",
    "\n",
    "        imgCrop = img[y - offset:y + h + offset, x - offset:x + w + offset]\n",
    "\n",
    "        # Resize the cropped hand region to match the input size of ResNet-18\n",
    "        imgResize = cv2.resize(imgCrop, (imgSize, imgSize))\n",
    "\n",
    "        # Normalize the image data \n",
    "        imgResize = transforms.ToTensor()(imgResize)\n",
    "        imgResize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])(imgResize)\n",
    "\n",
    "        # Make predictions using the ResNet-18 model\n",
    "        with torch.no_grad():\n",
    "            predictions = model(imgResize.unsqueeze(0))  # Add the batch dimension\n",
    "            predicted_label_index = torch.argmax(predictions, dim=1).item()\n",
    "            prediction = labels[predicted_label_index]\n",
    "\n",
    "            # Apply softmax to get confidence scores\n",
    "            softmax_scores = torch.softmax(predictions, dim=1)\n",
    "            confidence_score = softmax_scores[0][predicted_label_index].item()\n",
    "\n",
    "        confidence_percentage = confidence_score * 100\n",
    "        # Calculate the text size\n",
    "        text_to_display = f\"{prediction} ({confidence_percentage:.2f})\"\n",
    "        text_size, _ = cv2.getTextSize(text_to_display, cv2.FONT_HERSHEY_DUPLEX, font_scale, font_thickness)\n",
    "\n",
    "        # Calculate the width of the first rectangle based on the text size\n",
    "        rect_width = text_size[0] + 20  # Add some padding\n",
    "\n",
    "        # Draw the first rectangle\n",
    "        cv2.rectangle(imgOutput, (x - offset, y - offset - 40),\n",
    "                    (x - offset + rect_width, y - offset - 50 + 50), font_color, cv2.FILLED)\n",
    "\n",
    "        # Draw the text\n",
    "        cv2.putText(imgOutput, text_to_display, (x - offset + 10, y - 26), cv2.FONT_HERSHEY_DUPLEX, font_scale, (255, 255, 255), font_thickness)\n",
    "\n",
    "        # Draw the second rectangle\n",
    "        cv2.rectangle(imgOutput, (x - offset, y - offset),\n",
    "                    (x + w + offset, y + h + offset), font_color, 4)\n",
    "\n",
    "\n",
    "    cv2.imshow(\"Image\", imgOutput)\n",
    "    key = cv2.waitKey(1) \n",
    "    if key == ord('q'):\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXTRA CODE FOR PREPROCESSING ( BEFORE TRAINING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FRAME EXTRACTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "\n",
    "videos_dir = r'c:\\Users\\roksh\\OneDrive\\Desktop\\AI LAB\\SignLanguage\\BigDataset'\n",
    "\n",
    "frames_output_dir = r'C:\\Users\\roksh\\OneDrive\\Desktop\\AI LAB\\SignLanguage\\AddingExtracted'\n",
    "os.makedirs(frames_output_dir, exist_ok=True)\n",
    "\n",
    "# Create a mapping of category names to labels\n",
    "category_to_label = {category_name: label for label, category_name in enumerate(os.listdir(videos_dir))}\n",
    "\n",
    "# Iterate through video files and extract frames\n",
    "for category_name in os.listdir(videos_dir):\n",
    "    category_dir = os.path.join(videos_dir, category_name)\n",
    "    label = category_to_label[category_name]\n",
    "    \n",
    "    video_files = glob.glob(os.path.join(category_dir, '*.mp4'))\n",
    "    for video_file in video_files:\n",
    "        cap = cv2.VideoCapture(video_file)\n",
    "        frame_count = 0\n",
    "        \n",
    "        # Create a subdirectory for the video's frames\n",
    "        video_frames_dir = os.path.join(frames_output_dir, category_name, os.path.basename(video_file)[:-4])\n",
    "        os.makedirs(video_frames_dir, exist_ok=True)\n",
    "        \n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            frame_path = os.path.join(video_frames_dir, f'frame_{frame_count:04d}.jpg')\n",
    "            cv2.imwrite(frame_path, frame)\n",
    "            \n",
    "            frame_count += 1\n",
    "        \n",
    "        cap.release()\n",
    "\n",
    "print(\"Frame extraction and labeling completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA AUGMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "input_directory = r\"C:\\Users\\roksh\\OneDrive\\Desktop\\AI LAB\\SignLanguage\\FramesExtracted\"  \n",
    "output_directory = r\"C:\\Users\\roksh\\OneDrive\\Desktop\\AI LAB\\SignLanguage\\Augmented3\"  \n",
    "\n",
    "def apply_color_jitter(img):\n",
    "    # Apply slight color jitter\n",
    "    jitter_amount = np.random.randint(-10, 10)\n",
    "    img = img.astype(np.int16)\n",
    "    img += jitter_amount\n",
    "    img = np.clip(img, 0, 255).astype(np.uint8)\n",
    "    return img\n",
    "\n",
    "def apply_gaussian_noise(img):\n",
    "    # Add Gaussian noise\n",
    "    noise = np.random.normal(0, 10, img.shape).astype(np.int16)\n",
    "    img = img.astype(np.int16) + noise\n",
    "    img = np.clip(img, 0, 255).astype(np.uint8)\n",
    "    return img\n",
    "\n",
    "def apply_horizontal_flip(img):\n",
    "    return cv2.flip(img, 1)\n",
    "\n",
    "def apply_vertical_flip(img):\n",
    "    return cv2.flip(img, 0)\n",
    "\n",
    "def apply_background_removal(img):\n",
    "    fgbg = cv2.createBackgroundSubtractorMOG2()\n",
    "    fgmask = fgbg.apply(img)\n",
    "    masked_frame = cv2.bitwise_and(img, img, mask=fgmask)\n",
    "    return masked_frame\n",
    "\n",
    "def apply_random_rotation(img):\n",
    "    angle = random.randint(-60, 60)  # Rotate by random angle between -30 and 30 degrees\n",
    "    rows, cols, _ = img.shape\n",
    "    rotation_matrix = cv2.getRotationMatrix2D((cols/2, rows/2), angle, 1)\n",
    "    return cv2.warpAffine(img, rotation_matrix, (cols, rows))\n",
    "\n",
    "def apply_brightness_contrast(img):\n",
    "    alpha = random.uniform(0.8, 1.2)  # Brightness adjustment\n",
    "    beta = random.randint(-20, 20)    # Contrast adjustment\n",
    "    return cv2.convertScaleAbs(img, alpha=alpha, beta=beta)\n",
    "\n",
    "augmentation_functions = [\n",
    "    lambda img: apply_color_jitter(img),\n",
    "    lambda img: apply_gaussian_noise(img)\n",
    "    #lambda img: apply_background_removal(img)\n",
    "    #Add more augmentation functions here\n",
    "]\n",
    "\n",
    "for class_name in os.listdir(input_directory):\n",
    "    class_path = os.path.join(input_directory, class_name)\n",
    "    output_class_path = os.path.join(output_directory, class_name)\n",
    "    os.makedirs(output_class_path, exist_ok=True)\n",
    "    \n",
    "    for video_folder in os.listdir(class_path):\n",
    "        video_folder_path = os.path.join(class_path, video_folder)\n",
    "        output_video_folder_path = os.path.join(output_class_path, video_folder)\n",
    "        os.makedirs(output_video_folder_path, exist_ok=True)\n",
    "        \n",
    "        for frame_name in os.listdir(video_folder_path):\n",
    "            frame_path = os.path.join(video_folder_path, frame_name)\n",
    "            frame = cv2.imread(frame_path)\n",
    "            \n",
    "            # Apply random augmentation function to the frame\n",
    "            augmentation_func = random.choice(augmentation_functions)\n",
    "            augmented_frame = augmentation_func(frame)\n",
    "            \n",
    "            # Save augmented frame\n",
    "            output_frame_path = os.path.join(output_video_folder_path, f\"{frame_name.split('.')[0]}_augmented.jpg\")\n",
    "            cv2.imwrite(output_frame_path, augmented_frame)\n",
    "\n",
    "print(\"Data augmentation complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CROP HANDS OUT OF FRAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from cvzone.HandTrackingModule import HandDetector\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Initialize the HandDetector\n",
    "detector = HandDetector(maxHands=1)\n",
    "\n",
    "# Offset and image size parameters\n",
    "offset = 20\n",
    "imgSize = 300\n",
    "\n",
    "root_dir = r\"C:\\Users\\roksh\\OneDrive\\Desktop\\AI LAB\\SignLanguage\\provaa\" # CHANGE THIS\n",
    "output_root = r\"C:\\Users\\roksh\\OneDrive\\Desktop\\AI LAB\\SignLanguage\\Preprocessed_Frame_Hands\" #CHANGE THIS\n",
    "os.makedirs(output_root, exist_ok=True)\n",
    "\n",
    "categories = os.listdir(root_dir)\n",
    "\n",
    "for category in categories:\n",
    "    category_path = os.path.join(root_dir, category)\n",
    "\n",
    "    # Process each video within the category\n",
    "    videos = os.listdir(category_path)\n",
    "\n",
    "    for video in videos:\n",
    "        video_path = os.path.join(category_path, video)\n",
    "\n",
    "        # Create an output folder for the current video\n",
    "        output_category_folder = os.path.join(output_root, category)\n",
    "        output_video_folder = os.path.join(output_category_folder, video)\n",
    "        os.makedirs(output_video_folder, exist_ok=True)\n",
    "\n",
    "        # Process each frame in the video\n",
    "        frames = os.listdir(video_path)\n",
    "\n",
    "        for frame_filename in frames:\n",
    "            frame_path = os.path.join(video_path, frame_filename)\n",
    "\n",
    "            # Load the image\n",
    "            img = cv2.imread(frame_path)\n",
    "\n",
    "            # Find hands in the image\n",
    "            hands, img = detector.findHands(img)\n",
    "\n",
    "            if hands:\n",
    "                hand = hands[0]\n",
    "                x, y, w, h = hand['bbox']\n",
    "\n",
    "                imgWhite = np.ones((imgSize, imgSize, 3), np.uint8) * 255\n",
    "                imgCrop = img[y - offset:y + h + offset, x - offset:x + w + offset]\n",
    "\n",
    "                # Resize and place the cropped hand image into a white canvas\n",
    "                imgCropShape = imgCrop.shape\n",
    "                aspectRatio = h / w\n",
    "\n",
    "                if aspectRatio > 1:\n",
    "                    k = imgSize / h\n",
    "                    wCal = math.ceil(k * w)\n",
    "                    imgResize = cv2.resize(imgCrop, (wCal, imgSize))\n",
    "                    imgResizeShape = imgResize.shape\n",
    "                    wGap = math.ceil((imgSize - wCal) / 2)\n",
    "                    imgWhite[:, wGap:wCal + wGap] = imgResize\n",
    "                else:\n",
    "                    k = imgSize / w\n",
    "                    hCal = math.ceil(k * h)\n",
    "                    imgResize = cv2.resize(imgCrop, (imgSize, hCal))\n",
    "                    imgResizeShape = imgResize.shape\n",
    "                    hGap = math.ceil((imgSize - hCal) / 2)\n",
    "                    imgWhite[hGap:hCal + hGap, :] = imgResize\n",
    "\n",
    "                # Save the cropped hand image\n",
    "                output_filename = os.path.splitext(frame_filename)[0] + \"_hand.jpg\"\n",
    "                output_path = os.path.join(output_video_folder, output_filename)\n",
    "                cv2.imwrite(output_path, imgWhite)\n",
    "\n",
    "                print(f\"Saved cropped hand image: {output_path}\")\n",
    "\n",
    "print(\"All frames processed and cropped.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
