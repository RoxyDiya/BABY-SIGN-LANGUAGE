<p align="center"> 
  <img src="logo.png" alt="BSL Logo" width="600px" height="300px">
</p>
<h1 align="center"> Baby Sign Language Recognition </h1>
</br>

<p align="center"> 
  <img src="images/Signal.gif" alt="Sample signal" width="70%" height="70%">
</p>

<!-- TABLE OF CONTENTS -->
<h2 id="table-of-contents"> :book: Table of Contents</h2>

<details open="open">
  <summary>Table of Contents</summary>
  <ol>
    <li><a href="#about-the-project"> ➤ About The Project</a></li>
    <li><a href="#prerequisites"> ➤ Prerequisites</a></li>
    <li><a href="#dataset"> ➤ Dataset</a></li>
    <li><a href="#roadmap"> ➤ Roadmap</a></li>
    <!--<li><a href="#experiments">Experiments</a></li>-->
    <li><a href="#contributors"> ➤ Contributors</a></li>
  </ol>
</details>

![-----------------------------------------------------](https://raw.githubusercontent.com/andreasbm/readme/master/assets/lines/water.png)

<!-- ABOUT THE PROJECT -->
<h2 id="about-the-project"> :pencil: About The Project</h2>

<p align="justify"> 
  This project focuses on recognizing sign language gestures made by babies. It's aimed at understanding and interpreting communication through baby sign language. We explore the gestures infants make to convey their needs and emotions, with the goal of improving parent-infant communication and understanding.
</p>

<!--
<p align="center">
  <img src="logo.png" alt="Baby Sign Language" width="70%" height="70%">        
  <!--figcaption>Caption goes here</figcaption-->
<!-- </p> -->

![-----------------------------------------------------](https://raw.githubusercontent.com/andreasbm/readme/master/assets/lines/water.png)

<!-- PREREQUISITES -->
<h2 id="prerequisites"> :fork_and_knife: Prerequisites</h2>

[![made-with-python](https://img.shields.io/badge/Made%20with-Python-1f425f.svg)](https://www.python.org/) <br>
[![Made with Jupyter](https://img.shields.io/badge/Made%20with-Jupyter-orange?style=for-the-badge&logo=Jupyter)](https://jupyter.org/try) <br>

<!--This project is written in Python programming language. <br>-->
The following open source packages are used in this project:
* Numpy
* Torch
* Matplotlib
* Scikit-Learn
* Cv2
* CvZone

![-----------------------------------------------------](https://raw.githubusercontent.com/andreasbm/readme/master/assets/lines/water.png)

<!-- DATASET -->
<h2 id="dataset"> :floppy_disk: Dataset</h2>
<p> 
   Our dataset features video clips of infants performing sign gestures alongside their corresponding interpretations. These clips serve as the foundation for training models in recognizing the signs made by babies and understanding their intended meanings. Our mission is to enrich parent-infant communication through the interpretation of baby sign language.
</p>

<!--
<p align="center">
  <img src="images/Baby Sign Language Dataset.png" alt="Baby Sign Language Dataset" display="inline-block" width="60%" height="50%">
</p> -->

 **Expressions Included in the Dataset:**

- "milk"
- "eat"
- "I don't know"
- "down"
- "drink"
- "frustrated"
- "I love you"
- "mad/grumpy"
- "mine"
- "mom"
- "potty"
- "sorry" 

</p>

![-----------------------------------------------------](https://raw.githubusercontent.com/andreasbm/readme/master/assets/lines/water.png)

<!-- ROADMAP -->
<h2 id="roadmap"> :dart: Roadmap</h2>

<p align="justify"> 
  This roadmap outlines the journey from collecting data to deploying the baby sign language recognition program:

<ol>
  <li>
    <p align="justify"> 
      Data Gathering: We manually collected a diverse set of baby sign language videos from various sources, forming the foundation of our dataset.
    </p>
  </li>
  <li>
    <p align="justify"> 
      Data Augmentation: To enrich our dataset, we applied techniques such as rotation and noise addition, enhancing both its size and diversity.
    </p>
  </li>
  <li>
    <p align="justify"> 
      Data Preprocessing: This encompassed several essential tasks. We extracted frames from the videos, focusing on the hand gestures. 
    </p>
  </li>
  <li>
    <p align="justify"> 
      Model Training: With our dataset primed, we embarked on training deep learning models (CNN) to recognize and interpret baby sign gestures.
    </p>
  </li>
  <li>
    <p align="justify"> 
      Real-Time Testing: Our journey culminated in real-world applicability. We tested our trained models using webcam data, allowing us to understand and interpret baby sign language gestures, ultimately enhancing communication between parents and infants.
    </p>
  </li>
</ol>
</p>


![-----------------------------------------------------](https://raw.githubusercontent.com/andreasbm/readme/master/assets/lines/water.png)

<!-- CONTRIBUTORS -->

<h2 id="contributors"> :scroll: Contributors</h2>

<p>
  :mortar_board: <i>All participants in this project are undergraduate students of <a href="https://acsai.di.uniroma1.it/">Applied Computer Science and Artificial Intelligence</a> <b>@</b> <a href="https://www.uniroma1.it/en/">Sapienza University of Rome</a></i> <br> <br>
  
  :woman: <b>Rokshana Ahmed</b> <br>
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Email: <a>ahmed.1994927@studenti.uniroma1.it</a> <br>
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; GitHub: <a href="https://github.com/RoxyDiya">@RoxyDiya</a> <br>
  
  :woman: <b>Elena Martellucci</b> <br>
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Email: <a>martellucci.1988602@studenti.uniroma1.it</a> <br>
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; GitHub: <a href="https://github.com/elena-sapienza">@elena-sapienza</a> <br>

  :woman: <b>Firdaous Hajjaji</b> <br>
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Email: <a>hajjaji.2006406@studenti.uniroma1.it</a> <br>
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; GitHub: <a href="https://github.com/Firdaous2002">@Firdaous2002</a> <br>

</p>

<br>
✤ <i>This was the final project for the course AI LAB - Computer Vision at <a href="https://www.uniroma1.it/en/">Sapienza University of Rome</a><i>
